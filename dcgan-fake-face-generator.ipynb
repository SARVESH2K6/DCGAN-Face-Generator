{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":1328233,"sourceType":"datasetVersion","datasetId":770718}],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> **AI Face Generation with a Kaggle TPU** </center>\n\n***Welcome! This notebook contains a complete, end-to-end implementation of a Generative Adversarial Network (GAN) designed to create artificial human faces from scratch.***\n\n***This notebook is specifically configured to run on a **Kaggle TPU VM v3-8 accelerator** for maximum training speed. We will walk through every step, from loading the data to training the competing Generator and Discriminator models.***","metadata":{}},{"cell_type":"markdown","source":"## <center>**Step 1: Import Essential Libraries**</center>\n\n***First, we'll import all the necessary packages. This includes TensorFlow for building the model, Matplotlib for visualizing results, and helper libraries like PIL for image manipulation and `tqdm` for progress bars.***","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:02:14.704246Z","iopub.execute_input":"2025-09-08T09:02:14.704683Z","iopub.status.idle":"2025-09-08T09:02:14.715791Z","shell.execute_reply.started":"2025-09-08T09:02:14.704633Z","shell.execute_reply":"2025-09-08T09:02:14.710309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 2: Initialize the TPU Strategy**</center>\n\n***This is the most critical step for using a TPU. We create a `TPUStrategy` object that tells TensorFlow how to find and distribute the computations across all 8 cores of the TPU.***\n\n***All model building, loss definitions, and optimizer creation must be done within the scope of this strategy (`with strategy.scope():`) to ensure they are placed on the TPU hardware for accelerated, distributed training.***","metadata":{}},{"cell_type":"code","source":"try:\n    # Explicitly connect to the local TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n    \n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('✅ TPU initialized successfully.')\n\nexcept Exception as e:\n    print(f\"TPU initialization failed: {e}\")\n    # Fall back to the default strategy if TPU is not available\n    strategy = tf.distribute.get_strategy()\n\nprint(\"✅ REPLICAS: \", strategy.num_replicas_in_sync)\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:47:05.106775Z","iopub.execute_input":"2025-09-08T08:47:05.107340Z","iopub.status.idle":"2025-09-08T08:47:12.809578Z","shell.execute_reply.started":"2025-09-08T08:47:05.107309Z","shell.execute_reply":"2025-09-08T08:47:12.804203Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 3: Configure Project Parameters**</center>\n\n***Here, we define all the key constants for our project. This includes image dimensions, the path to our dataset, and crucial training hyperparameters.***\n\n***For TPU training, the `GLOBAL_BATCH_SIZE` is the most important parameter. It is calculated by multiplying the `BATCH_SIZE_PER_REPLICA` (the number of images processed by a single TPU core) by the number of `REPLICAS` (which is 8). This ensures all TPU cores are kept busy, maximizing training speed.***","metadata":{}},{"cell_type":"code","source":"# Image dimensions\nIMAGE_WIDTH = 128\nIMAGE_HEIGHT = 128\nIMAGE_CHANNELS = 3\nIMAGE_SHAPE = (IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)\n\n# Dataset path\nDATASET_PATH = '/kaggle/input/face-mask-lite-dataset/without_mask'\n\n# The size of the random noise vector (latent space) for the generator\nLATENT_DIM = 100\n\n# --- TPU-Specific Training Configuration ---\nBATCH_SIZE_PER_REPLICA = 128\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * REPLICAS\nEPOCHS = 800\n\nprint(f\"Batch size per replica: {BATCH_SIZE_PER_REPLICA}\")\nprint(f\"Global batch size (for all {REPLICAS} replicas): {GLOBAL_BATCH_SIZE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:57:25.213407Z","iopub.execute_input":"2025-09-08T14:57:25.214115Z","iopub.status.idle":"2025-09-08T14:57:25.301086Z","shell.execute_reply.started":"2025-09-08T14:57:25.214081Z","shell.execute_reply":"2025-09-08T14:57:25.299820Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 4: Load and Prepare the Dataset**</center>\n\n***In this step, we load the images from the disk and prepare them for training. This involves several key actions:***\n\n***1.  Reading each image file from the specified directory.***\n\n***2.  Resizing it to our standard 128x128 dimensions.***\n\n***3.  Normalizing the pixel values from the original `[0, 255]` range to `[-1, 1]`. This is a critical step that helps stabilize GAN training.***\n\n***4.  Creating a high-performance `tf.data.Dataset` pipeline.This object efficiently shuffles, batches, and prefetches the data, ensuring the TPU is never idle waiting for the next batch of images.***","metadata":{}},{"cell_type":"code","source":"def load_images_from_path(path, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT)):\n    \"\"\"Loads all images from a directory, resizes, and normalizes them.\"\"\"\n    image_list = []\n    print(f\"Loading images from: {path}\")\n\n    # Use tqdm for a progress bar\n    for filename in tqdm(os.listdir(path)):\n        try:\n            img_path = os.path.join(path, filename)\n            # Open the image using PIL, convert to RGB, and resize\n            img = Image.open(img_path).convert('RGB').resize(target_size)\n            image_list.append(np.asarray(img))\n        except Exception as e:\n            print(f\"\\nSkipping file {filename} due to error: {e}\")\n\n    # Convert the list of images to a single NumPy array\n    images_np = np.array(image_list, dtype='float32')\n\n    # Normalize the images to the range [-1, 1]. This is crucial for GAN stability.\n    images_np = (images_np - 127.5) / 127.5\n    return images_np\n\n# --- Create the Data Pipeline for the TPU ---\n\n# 1. Load the images into a NumPy array.\nX_train = load_images_from_path(DATASET_PATH)\nprint(f\"\\nDataset loaded into memory. Shape of image array: {X_train.shape}\")\n\n# 2. Create a TensorFlow Dataset object.\ntrain_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n\n# 3. Shuffle, Batch, and Prefetch for high performance.\n#   .shuffle(): Randomizes the order of images each epoch for better training.\n#   .batch(): Groups images into the large GLOBAL_BATCH_SIZE we defined.\n#   .prefetch(): Pre-loads the next batch onto the TPU while the current one is processing.\n#   .cache(): Caches the dataset in memory after the first epoch to speed up subsequent epochs.\ntrain_dataset = train_dataset.cache().shuffle(buffer_size=X_train.shape[0]).batch(GLOBAL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\nprint(f\"✅ Data pipeline created successfully and is ready for training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:47:44.611924Z","iopub.execute_input":"2025-09-08T08:47:44.612276Z","iopub.status.idle":"2025-09-08T08:59:08.923563Z","shell.execute_reply.started":"2025-09-08T08:47:44.612250Z","shell.execute_reply":"2025-09-08T08:59:08.919101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 5: Build the Generator**</center>\n\n***Now we'll build the first of our two competing models: the **Generator**. This model acts as the \"artist.\" Its job is to take a small vector of random numbers (from the \"latent space\") and learn how to transform it into a complex, 128x128 image that looks like a real human face.***\n\n***The architecture works by progressively upsampling the input through a series of `Conv2DTranspose` layers, which are essentially the reverse of a standard convolutional layer. The `tanh` activation on the final layer is important because it ensures the output pixel values are in the `[-1, 1]` range, perfectly matching the normalization of our real images.***\n\n***Crucially, we define the model within the `strategy.scope()` to ensure it is created on the TPU hardware for accelerated training.***","metadata":{}},{"cell_type":"code","source":"def build_generator(latent_dim=LATENT_DIM):\n    \"\"\"\n    Creates the Generator model.\n    It takes a random noise vector and upsamples it to a 128x128x3 image.\n    \"\"\"\n    model = Sequential(name='Generator')\n\n    # Start with a dense layer to project the noise vector into a suitable shape\n    # We'll start with a small 8x8 image with 1024 filters\n    model.add(layers.Dense(8 * 8 * 1024, input_dim=latent_dim))\n    model.add(layers.Reshape((8, 8, 1024)))\n\n    # --- Upsampling Block 1: 8x8 -> 16x16 ---\n    model.add(layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Upsampling Block 2: 16x16 -> 32x32 ---\n    model.add(layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Upsampling Block 3: 32x32 -> 64x64 ---\n    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Upsampling Block 4: 64x64 -> 128x128 ---\n    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Output Layer ---\n    # Final layer to produce the 128x128 image with 3 color channels (RGB)\n    # The 'tanh' activation function squashes the output pixel values to be between -1 and 1,\n    # matching the normalization of our real training images.\n    model.add(layers.Conv2DTranspose(3, kernel_size=5, padding='same', activation='tanh'))\n\n    return model\n\n# --- Create the Generator within the TPU strategy scope ---\nwith strategy.scope():\n    generator = build_generator()\n\nprint(\"✅ Generator model built successfully.\")\ngenerator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:59:14.713970Z","iopub.execute_input":"2025-09-08T08:59:14.714303Z","iopub.status.idle":"2025-09-08T08:59:18.710201Z","shell.execute_reply.started":"2025-09-08T08:59:14.714276Z","shell.execute_reply":"2025-09-08T08:59:18.704972Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 6: Build the Discriminator**</center>\n\n***Now we build the second model, the **Discriminator**. This model acts as the \"art critic.\" It's a standard convolutional neural network (CNN) designed for image classification.***\n\n***Its job is to take an image (either a real one from our dataset or a fake one from the Generator) and output a single probability score indicating how likely it thinks the image is to be real. It does this by downsampling the image using `Conv2D` layers to extract key features. The `Dropout` layer is included to help stabilize training by preventing the critic from overpowering the artist too quickly.***","metadata":{}},{"cell_type":"code","source":"def build_discriminator(image_shape=IMAGE_SHAPE):\n    \"\"\"\n    Creates the Discriminator model.\n    It's a CNN that takes a 128x128x3 image and classifies it as real (output ~1) or fake (output ~0).\n    \"\"\"\n    model = Sequential(name='Discriminator')\n\n    # --- Downsampling Block 1: 128x128 -> 64x64 ---\n    model.add(layers.GaussianNoise(0.1, input_shape=image_shape))\n    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Downsampling Block 2: 64x64 -> 32x32 ---\n    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Downsampling Block 3: 32x32 -> 16x16 ---\n    model.add(layers.Conv2D(256, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n    \n    # --- Downsampling Block 4: 16x16 -> 8x8 ---\n    model.add(layers.Conv2D(512, kernel_size=4, strides=2, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU(alpha=0.2))\n\n    # --- Final Layers for Classification ---\n    # Flatten the feature maps into a single vector\n    model.add(layers.Flatten())\n    \n    # Dropout layer to prevent the discriminator from becoming too powerful too quickly.\n    model.add(layers.Dropout(0.4))\n    \n    # Output layer: A single neuron with a sigmoid activation to give a probability (0 to 1).\n    model.add(layers.Dense(1, activation='sigmoid'))\n\n    return model\n\n# --- Create the Discriminator within the TPU strategy scope ---\nwith strategy.scope():\n    discriminator = build_discriminator()\n\nprint(\"✅ Discriminator model built successfully.\")\ndiscriminator.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T08:59:48.843933Z","iopub.execute_input":"2025-09-08T08:59:48.844273Z","iopub.status.idle":"2025-09-08T08:59:49.696318Z","shell.execute_reply.started":"2025-09-08T08:59:48.844244Z","shell.execute_reply":"2025-09-08T08:59:49.691161Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 7: Define Loss Functions and Optimizers**</center>\n\n***Now we define the \"rules of the game.\" This involves setting up the loss functions and optimizers for both models.***\n\n-   ***Loss Function: We use `BinaryCrossentropy`, which is perfect for a classification task with two outcomes (in our case, \"real\" or \"fake\"). The **Discriminator's loss** is a combination of its error on real images and its error on fake images. The **Generator's loss** is based purely on how well it manages to fool the Discriminator.***\n  \n-   ***Optimizers: We use the `Adam` optimizer, a popular and effective choice for GANs. We create two separate instances—one for each model—so they can learn independently.***\n  \n-   ***TPU Scope: Crucially, these components are defined within the `strategy.scope()` to ensure they are created on the TPU for distributed training. The `tf.nn.compute_average_loss` function is used to correctly average the loss calculated across all 8 TPU cores.***","metadata":{}},{"cell_type":"code","source":"# This block is mandatory. It ensures that the optimizers and loss function\n# are created on the TPU, allowing for distributed training.\nwith strategy.scope():\n    # --- Loss Function ---\n    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n                                                       reduction=tf.keras.losses.Reduction.NONE)\n\n    def discriminator_loss(real_output, fake_output):\n        # The discriminator wants to classify real images as 1 and fake images as 0.\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n        total_loss = real_loss + fake_loss\n        # We must scale the loss by the global batch size for proper gradient updates on the TPU.\n        return tf.nn.compute_average_loss(total_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n\n    def generator_loss(fake_output):\n        # The generator's goal is to fool the discriminator. It wants the discriminator\n        # to classify its fake images as 1 (real).\n        loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n        # Scale the loss by the global batch size.\n        return tf.nn.compute_average_loss(loss, global_batch_size=GLOBAL_BATCH_SIZE)\n\n\n    # --- Optimizers ---\n    # Adam is the go-to optimizer for GANs. It's efficient and works well.\n    # We use a slightly lower learning rate (lr) and a beta_1 term of 0.5,\n    # which are common practices that help stabilize GAN training.\n    # We need two separate optimizers because we train the two networks independently.\n    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004, beta_1=0.5)\n\nprint(\"✅ Loss functions and optimizers defined successfully within the TPU scope.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:00:16.299345Z","iopub.execute_input":"2025-09-08T09:00:16.299705Z","iopub.status.idle":"2025-09-08T09:00:16.358835Z","shell.execute_reply.started":"2025-09-08T09:00:16.299662Z","shell.execute_reply":"2025-09-08T09:00:16.355259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 8: Define the Core Training Step**</center>\n\n***This is the heart of our GAN. We wrap the logic for a single training step inside a `@tf.function` decorator. This special command tells TensorFlow to compile the Python code into a highly optimized, high-performance graph, which is essential for getting maximum speed from the TPU.***\n\n***The logic involves:***\n\n***1.  Generating fake images.***\n\n***2.  Asking the Discriminator to classify both real and fake images.***\n\n***3.  Calculating the loss for both models.***\n\n***4.  Calculating the gradients and updating the model weights.***\n\n***The `distributed_train_step` function then acts as a manager, using `strategy.run` to execute this core `train_step` on all TPU cores in parallel and `strategy.reduce` to gather the results.***","metadata":{}},{"cell_type":"code","source":"# The @tf.function decorator compiles the function into a high-performance TensorFlow graph.\n@tf.function\ndef train_step(images):\n    \"\"\"Executes a single training step on one batch of images.\"\"\"\n    \n    # Dynamically get the per-replica batch size from the input tensor's shape.\n    batch_size = tf.shape(images)[0]\n    noise = tf.random.normal([batch_size, LATENT_DIM])\n\n    # Use tf.GradientTape to record the operations for automatic differentiation.\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # 1. Generate a batch of fake images\n        generated_images = generator(noise, training=True)\n\n        # 2. Get the discriminator's predictions\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        # 3. Calculate the loss for each model\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    # 4. Calculate and apply gradients\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss\n\n\n@tf.function\ndef distributed_train_step(dataset_inputs):\n    \"\"\"Executes the train_step function on each of the TPU replicas.\"\"\"\n    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n    \n    gen_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[0], axis=None)\n    disc_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses[1], axis=None)\n    \n    return gen_loss, disc_loss\n\n\nprint(\"✅ Core training step functions re-defined and compiled for the TPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:03:38.185056Z","iopub.execute_input":"2025-09-08T09:03:38.185399Z","iopub.status.idle":"2025-09-08T09:03:38.201534Z","shell.execute_reply.started":"2025-09-08T09:03:38.185372Z","shell.execute_reply":"2025-09-08T09:03:38.196415Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 9: Create Image Saving Utilities**</center>\n\n***To monitor our GAN's progress during the long training process, it's essential to visualize its output. This helper function will generate a grid of sample images from the Generator and save it to a file.***\n\n***We use a `fixed_seed` (a constant noise vector that doesn't change) to generate these samples. By giving the Generator the same starting point every time, we can clearly see how its ability to create a face from that seed evolves and improves over the epochs. This is a much better way to gauge progress than looking at completely random new images each time.***","metadata":{}},{"cell_type":"code","source":"# Create a directory to save the generated images during training\nif not os.path.exists('gan_images'):\n    os.makedirs('gan_images')\n\ndef save_plot(images, epoch, n=4):\n    \"\"\"\n    Generates a plot of n x n images and saves it to a file.\n    The pixel values are rescaled from [-1, 1] to [0, 1] for plotting.\n    \"\"\"\n    images = (images + 1) / 2.0  # Rescale from [-1, 1] to [0, 1]\n    fig, axs = plt.subplots(n, n, figsize=(8, 8))\n    for i in range(n * n):\n        ax = axs[i // n, i % n]\n        ax.imshow(images[i])\n        ax.axis('off')\n    fig.savefig(f\"gan_images/generated_plot_epoch-{epoch+1:03d}.png\")\n    plt.close(fig)\n\n\n# We'll use a fixed random seed to see how the same starting noise evolves over time.\n# This makes it easier to judge the generator's progress.\nfixed_seed = tf.random.normal([16, LATENT_DIM])\n\nprint(\"✅ Helper function for saving images created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:01:11.874482Z","iopub.execute_input":"2025-09-08T09:01:11.874852Z","iopub.status.idle":"2025-09-08T09:01:11.893130Z","shell.execute_reply.started":"2025-09-08T09:01:11.874820Z","shell.execute_reply":"2025-09-08T09:01:11.886652Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 10: The Main Training Loop**</center>\n\n***This is the final step where we bring everything together and start training. We loop for the specified number of `EPOCHS`. In each epoch, we iterate through every batch of our `train_dataset`, calling our `distributed_train_step` function to update the models based on their performance.***\n\n***We'll print the average loss for both models at the end of each epoch and save our sample images every 5 epochs so we can watch our GAN learn to create faces in near real-time.***","metadata":{}},{"cell_type":"code","source":"def train_gan(dataset, epochs):\n    \"\"\"The main function to train the GAN models.\"\"\"\n    # Record the start time\n    start = time.time()\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        \n        # Initialize loss trackers for the epoch\n        gen_loss_epoch = 0\n        disc_loss_epoch = 0\n        num_batches = 0\n\n        # Iterate over each batch in the dataset.\n        # The 'train_dataset' we created handles the batching automatically.\n        for image_batch in tqdm(dataset, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            # Execute one distributed training step and get the losses.\n            g_loss, d_loss = distributed_train_step(image_batch)\n            \n            # Accumulate the losses\n            gen_loss_epoch += g_loss\n            disc_loss_epoch += d_loss\n            num_batches += 1\n\n        # --- End of Epoch ---\n        # Calculate the average loss for the epoch\n        avg_gen_loss = gen_loss_epoch / num_batches\n        avg_disc_loss = disc_loss_epoch / num_batches\n        \n        epoch_time = time.time() - epoch_start\n        \n        print(f\"Time for epoch {epoch + 1} is {epoch_time:.2f} sec\")\n        print(f\"    Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}\")\n\n        # Generate and save a grid of images every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            print(\"    Generating and saving sample images...\")\n            # Generate images from the fixed seed\n            predictions = generator(fixed_seed, training=False)\n            # Save the plot\n            save_plot(predictions, epoch)\n\n    # --- End of Training ---\n    total_time = time.time() - start\n    print(f\"\\n🎉 Training finished in {total_time / 60:.2f} minutes. 🎉\")\n\n\n# --- Let's start the training! ---\nprint(\"Starting GAN training on the TPU...\")\ntrain_gan(train_dataset, EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:19:30.585437Z","iopub.execute_input":"2025-09-08T09:19:30.585780Z","iopub.status.idle":"2025-09-08T09:20:05.402289Z","shell.execute_reply.started":"2025-09-08T09:19:30.585752Z","shell.execute_reply":"2025-09-08T09:20:05.398251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 11: Generate Final Images**</center>\n\n***After the training is complete, the `generator` variable in our notebook holds the fully trained \"artist.\" We can now use it to create new, unique faces on demand.***\n\n***This cell generates a 5x5 grid of completely new faces by feeding brand new random noise vectors into the generator, showcasing the final result of our training process.***","metadata":{}},{"cell_type":"code","source":"# Create a new plot to display the final results\nfig, axs = plt.subplots(5, 5, figsize=(12, 12))\nfig.suptitle(\"Newly Generated Faces from Trained Model\", fontsize=20)\n\nfor i in range(5 * 5):\n    # 1. Generate a new random noise vector\n    # We use tf.random.normal to create a single noise vector of size LATENT_DIM\n    noise = tf.random.normal([1, LATENT_DIM])\n    \n    # 2. Pass the noise to the generator. `training=False` is important here.\n    generated_image = generator(noise, training=False)\n    \n    # 3. Post-process the image for display\n    # The generator outputs pixel values from [-1, 1]. We rescale them to [0, 1]\n    # so matplotlib can display the image correctly.\n    img_display = (generated_image[0].numpy() + 1) / 2.0\n    \n    # 4. Plot the image\n    ax = axs[i // 5, i % 5]\n    ax.imshow(img_display)\n    ax.axis('off')\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T09:17:36.099849Z","iopub.execute_input":"2025-09-08T09:17:36.100152Z","iopub.status.idle":"2025-09-08T09:17:39.708789Z","shell.execute_reply.started":"2025-09-08T09:17:36.100125Z","shell.execute_reply":"2025-09-08T09:17:39.702578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <center>**Step 12: Save the Final Model**</center>\n\n***Saving the model allows us to load it later in a different notebook or application for inference, so we can generate new faces anytime without having to retrain the model from scratch.***","metadata":{}},{"cell_type":"code","source":"generator.save('face_generator_model.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}